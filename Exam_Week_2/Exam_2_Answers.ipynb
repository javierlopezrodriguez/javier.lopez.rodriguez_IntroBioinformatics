{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exam 2 Answers by Javier López Rodríguez  (javier.lopez.rodriguez@alumnos.upm.es)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1:  Controls\n",
    "\n",
    "Write a Python script that proves that the lines of data in Germplasm.tsv, and LocusGene are in the same sequence, based on the AGI Locus Code (ATxGxxxxxx).  (hint: This will help you decide how to load the data into the database)\n",
    "\n",
    "---------\n",
    "\n",
    "### Explanation for problem 1:\n",
    "\n",
    "We are dealing with .tsv files with headers. In order to read them, I'll use csv.DictReader because it is more efficient and easier (and I haven't used it before this course, unlike the normal python input/output, so it helps me practice new things).\n",
    "\n",
    "After opening both of the files, I first iterate through each file one time because I haven't found a method of csv.DictReader that returns its length. The object csv.DictReader is an iterator, so it doesn't load every line to memory at the same time, unlike a list. Therefore, converting it into a list in order to use len() is a memory dangerous process, so I avoided doing that.\n",
    "\n",
    "Then, I reset the pointer to the start and create the csv.DictReader objects for both files.\n",
    "\n",
    "Because we want to prove that both files have the same locus code in the same line, the number of lines should be the same in both cases. I check this (that's why I need to iterate the first time for the lengths). If the lengths aren't the same, it will only compare until the smallest file ends (minimum of length1 and length2).\n",
    "\n",
    "I create two lists for storing the line number of matches and mismatches (to check at the end if there is any mismatch, how many there are, and if the number of matches and mismatches is what we expected).\n",
    "\n",
    "Then, in a for loop, I iterate through both DictReaders at the same time, using the zip function. In case of different number of entries, zip stops when the smallest one ends (this is equivalent to iterating until the minimum of length1 and length2).\n",
    "\n",
    "I go through every pair of entries, store the Locus code (which DictReader makes very easy because each row is a dictionary and there's no need to split/do anything else), and compare both codes. I append the line number to either list, depending on it being a match or a mismatch.\n",
    "\n",
    " * Note that zip creates an iterator, not a list. Therefore, a zip of two DictReaders still does not load everything into memory at the same time, so the advantage of using DictReaders is maintained.\n",
    "\n",
    "In the end, I check if there are any mismatches (and print them), print the number of matches, and have a third condition just in case the number of matches + mismatches is less than the number of compared lines. The latter is just a precaution because (I think) it should never happen unless the code is incorrect.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Both files have the same number of lines: 32 (without header).\n",
      "No mismatches found. There were 32 lines with matching Locus code.\n",
      "Execution time in seconds, with my normal solution: 0.003720998764038086\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import time # I wanted to test how much time it took to run and compare it with my alternative solution (below)\n",
    "\n",
    "filename1 = \"LocusGene.tsv\"\n",
    "filename2 = \"Germplasm.tsv\"\n",
    "\n",
    "start_time = time.time() # we store the start time\n",
    "\n",
    "with open(filename1) as file1, open(filename2) as file2:\n",
    "        \n",
    "    # getting the length of each file iterating through each line and adding 1 for each line\n",
    "    # substracting 1 so that we don't count the header\n",
    "    length1 = sum(1 for _ in file1) - 1 \n",
    "    length2 = sum(1 for _ in file2) - 1 \n",
    "\n",
    "    # reset the pointer to the start of each file\n",
    "    file1.seek(0) \n",
    "    file2.seek(0)\n",
    "\n",
    "    # opening each file with csv.DictReader\n",
    "    locusgene = csv.DictReader(file1, delimiter=\"\\t\", quotechar='\"') # default fieldnames because of the header\n",
    "    germplasm = csv.DictReader(file2, delimiter=\"\\t\", quotechar='\"') # default fieldnames because of the header\n",
    "\n",
    "    if length1 != length2: # different number of lines\n",
    "        print(\"Warning: There are not the same number of lines in both files ({} and {})\".format(length1, length2))\n",
    "        print(\"Only the first {} lines of each file will be compared.\".format(min(length1, length2)))\n",
    "    else: # equal number of lines\n",
    "        print(\"Both files have the same number of lines: {} (without header).\".format(length1))\n",
    "\n",
    "    mismatched_lines = [] # will store the indexes of the mismatched lines, if any\n",
    "    correct_lines = [] # will store the indexes of the correct lines\n",
    "\n",
    "    # iterating through every pair of elements\n",
    "    linenumber = 1 # keeps track of the line number we're in, starts in 1 (because we skip the header using DictReader)\n",
    "    for entry1, entry2 in zip(locusgene, germplasm): # iterating through both DictReaders at the same time\n",
    "        locus1, locus2 = entry1[\"Locus\"], entry2[\"Locus\"]\n",
    "        #print(locus1 + \" \" + locus2) # checking that locus1 and locus2 contain the expected strings\n",
    "        # checking if they match or mismatch\n",
    "        if locus1 == locus2: # match\n",
    "            correct_lines.append(linenumber)\n",
    "        else: # mismatch\n",
    "            mismatched_lines.append(linenumber)\n",
    "        linenumber += 1 # increment linenumber\n",
    "        \n",
    "    if len(mismatched_lines) > 0: # there are mismatches, output them\n",
    "        print(\"Warning: There are some mismatches.\")\n",
    "        print(\"Mismatched lines: \" + \" \".join(mismatched_lines))\n",
    "        print(\"There were {} lines with matching Locus code.\".format(len(correct_lines)))\n",
    "    elif len(correct_lines) == min(length1, length2): # there are no mismatches and every line checked was a match\n",
    "        print(\"No mismatches found. There were {} lines with matching Locus code.\".format(len(correct_lines)))\n",
    "    else: # there are no mismatches but not every line checked was a match -> this should never happen\n",
    "        print(\"Error: there were less matches than expected. Something went wrong.\")\n",
    "\n",
    "# using \"with open(...) as ...\", we don't need to close the files afterwards, it is done automatically.\n",
    "\n",
    "execution_time = (time.time() - start_time) # we compute the time that has passed\n",
    "print('Execution time in seconds, with my normal solution: ' + str(execution_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative solution for problem 1:\n",
    "This problem could also be solved using the pandas library, although this way we load the entire content of the files into dataframes, which could impact memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of items in LocusGene.tsv is 32\n",
      "Number of items in Germplasm.tsv is 32\n",
      "Number of matching Locus codes: 32\n",
      "Execution time in seconds, with my alternative solution (using pandas): 0.014252901077270508\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "filename1 = \"LocusGene.tsv\"\n",
    "filename2 = \"Germplasm.tsv\"\n",
    "\n",
    "start_time = time.time() # we store the start time\n",
    "\n",
    "# reading the .tsv into pandas dataframes\n",
    "df1 = pd.read_csv(filename1, sep = \"\\t\")\n",
    "df2 = pd.read_csv(filename2, sep = \"\\t\")\n",
    "\n",
    "# renaming the columns so that, when concatenating the columns, they are named differently\n",
    "df1 = df1.rename(columns = {\"Locus\": \"Locus1\"})\n",
    "df2 = df2.rename(columns = {\"Locus\": \"Locus2\"})\n",
    "\n",
    "# printing number of items (size) of each column\n",
    "print(\"Number of items in \" + filename1 + \" is {}\".format(df1[\"Locus1\"].size))\n",
    "print(\"Number of items in \" + filename2 + \" is {}\".format(df2[\"Locus2\"].size))\n",
    "\n",
    "# concatenating both columns so that the following comparison can be made\n",
    "# if the number of elements is different, pd.concat adds NaN to the missing elements of the smallest column\n",
    "# so that both columns have the same length\n",
    "dfconcat = pd.concat([df1[\"Locus1\"], df2[\"Locus2\"]], axis = 1)\n",
    "\n",
    "# comparing the contents of both columns\n",
    "# the comparison gives a boolean array, the sum of that array is the number of True elements (number of matches)\n",
    "# doing this without concatenating both columns first gives an error if the number of elements is different,\n",
    "# that is why we need to concatenate both columns first into the same data frame\n",
    "print(\"Number of matching Locus codes: \" + str(sum(dfconcat[\"Locus1\"] == dfconcat[\"Locus2\"])))\n",
    "\n",
    "execution_time = (time.time() - start_time) # we compute the time that has passed\n",
    "print('Execution time in seconds, with my alternative solution (using pandas): ' + str(execution_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In my case, the execution time for the first solution was 0.004 seconds, and the second solution using pandas was 0.014 seconds. So, for these two files, my first solution is more efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2:  Design and create the database\n",
    "* It should have two tables - one for each of the two data files.\n",
    "* The two tables should be linked in a 1:1 relationship\n",
    "* you may use either sqlMagic or pymysql to build the database\n",
    "\n",
    "---------\n",
    "\n",
    "### Explanation for problem 2:\n",
    "\n",
    "I'm using sqlMagic because I find it easier for creating databases and tables.\n",
    "\n",
    "We know that both files contain the same AGI Locus codes in the same positions, and both tables are going to have that field. \n",
    "\n",
    "Because the relationship between the two tables is 1:1 and the AGI Locus code in this case is a unique identifier of each entry in both tables, I am going to use it as the primary key of both tables. Therefore, the tables won't include additional numeric ids. Linking one table with the other in queries that involve both is going to happen via the AGI Locus codes.\n",
    "\n",
    "When creating the table locusgene:\n",
    "\n",
    "    'locus' is the primary key, a not null string of at most 10 characters,\n",
    "    'gene' is a not null string of at most 10 characters,\n",
    "    'protein_length' is a not null integer.\n",
    "    \n",
    "When creating the table germplasm:\n",
    "\n",
    "    'locus' is the primary key, a not null string of at most 10 characters (and the same as in the above table),\n",
    "    'germplasm' is a not null string of at most 50 characters,\n",
    "    'phenotype' is a not null string of at most 500 characters,\n",
    "    'pubmed' is a not null integer.\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Connected: root@mysql'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Connecting to sqlMagic\n",
    "%load_ext sql\n",
    "#%config SqlMagic.autocommit=False\n",
    "%sql mysql+pymysql://root:root@127.0.0.1:3306/mysql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * mysql+pymysql://root:***@127.0.0.1:3306/mysql\n",
      "2 rows affected.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#%sql drop database examweek2; # dropping the database in case it already exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * mysql+pymysql://root:***@127.0.0.1:3306/mysql\n",
      "1 rows affected.\n",
      " * mysql+pymysql://root:***@127.0.0.1:3306/mysql\n",
      "0 rows affected.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating the database examweek2:\n",
    "%sql create database examweek2;\n",
    "# In order to interact with the database:\n",
    "%sql use examweek2;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * mysql+pymysql://root:***@127.0.0.1:3306/mysql\n",
      "0 rows affected.\n",
      " * mysql+pymysql://root:***@127.0.0.1:3306/mysql\n",
      "0 rows affected.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating the table locusgene\n",
    "%sql CREATE TABLE locusgene (locus VARCHAR(10) NOT NULL PRIMARY KEY, \\\n",
    "                             gene VARCHAR(10) NOT NULL, \\\n",
    "                             protein_length INTEGER NOT NULL);\n",
    "# Creating the table germplasm\n",
    "%sql CREATE TABLE germplasm (locus VARCHAR(10) NOT NULL PRIMARY KEY, \\\n",
    "                             germplasm VARCHAR(50) NOT NULL, \\\n",
    "                             phenotype VARCHAR(500) NOT NULL, \\\n",
    "                             pubmed INTEGER NOT NULL);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * mysql+pymysql://root:***@127.0.0.1:3306/mysql\n",
      "3 rows affected.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>Field</th>\n",
       "        <th>Type</th>\n",
       "        <th>Null</th>\n",
       "        <th>Key</th>\n",
       "        <th>Default</th>\n",
       "        <th>Extra</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>locus</td>\n",
       "        <td>varchar(10)</td>\n",
       "        <td>NO</td>\n",
       "        <td>PRI</td>\n",
       "        <td>None</td>\n",
       "        <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>gene</td>\n",
       "        <td>varchar(10)</td>\n",
       "        <td>NO</td>\n",
       "        <td></td>\n",
       "        <td>None</td>\n",
       "        <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>protein_length</td>\n",
       "        <td>int(11)</td>\n",
       "        <td>NO</td>\n",
       "        <td></td>\n",
       "        <td>None</td>\n",
       "        <td></td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[('locus', 'varchar(10)', 'NO', 'PRI', None, ''),\n",
       " ('gene', 'varchar(10)', 'NO', '', None, ''),\n",
       " ('protein_length', 'int(11)', 'NO', '', None, '')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%sql DESCRIBE locusgene # Describing the tables to check that they are correctly made"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * mysql+pymysql://root:***@127.0.0.1:3306/mysql\n",
      "4 rows affected.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>Field</th>\n",
       "        <th>Type</th>\n",
       "        <th>Null</th>\n",
       "        <th>Key</th>\n",
       "        <th>Default</th>\n",
       "        <th>Extra</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>locus</td>\n",
       "        <td>varchar(10)</td>\n",
       "        <td>NO</td>\n",
       "        <td>PRI</td>\n",
       "        <td>None</td>\n",
       "        <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>germplasm</td>\n",
       "        <td>varchar(50)</td>\n",
       "        <td>NO</td>\n",
       "        <td></td>\n",
       "        <td>None</td>\n",
       "        <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>phenotype</td>\n",
       "        <td>varchar(500)</td>\n",
       "        <td>NO</td>\n",
       "        <td></td>\n",
       "        <td>None</td>\n",
       "        <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>pubmed</td>\n",
       "        <td>int(11)</td>\n",
       "        <td>NO</td>\n",
       "        <td></td>\n",
       "        <td>None</td>\n",
       "        <td></td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[('locus', 'varchar(10)', 'NO', 'PRI', None, ''),\n",
       " ('germplasm', 'varchar(50)', 'NO', '', None, ''),\n",
       " ('phenotype', 'varchar(500)', 'NO', '', None, ''),\n",
       " ('pubmed', 'int(11)', 'NO', '', None, '')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%sql DESCRIBE germplasm # Describing the tables to check that they are correctly made"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3: Fill the database\n",
    "Using pymysql, create a Python script that reads the data from these files, and fills the database.  There are a variety of strategies to accomplish this.  I will give all strategies equal credit - do whichever one you are most confident with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation for problem 3:\n",
    "\n",
    "First, we import pymysql.cursors and connect to the examweek2 database, making sure that the character set is UTF8 and that autocommit is set to True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing pymysql.cursors and connecting to the database\n",
    "import pymysql.cursors\n",
    "\n",
    "# Connecting to the database examweek2\n",
    "connection = pymysql.connect(host='localhost',\n",
    "                             user='root',\n",
    "                             password='root',\n",
    "                             db='examweek2', # database name\n",
    "                             charset='utf8mb4',  \n",
    "                             cursorclass=pymysql.cursors.DictCursor,\n",
    "                             autocommit = True) # I'm setting autocommit to True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the design I've chosen for the database, because the relationship is 1:1 and both of them have the same primary key and no additional id, it doesn't matter which table we fill out first. We don't need to keep track of the last inserted id (because there is none).\n",
    "\n",
    "In order to avoid repetition, I first created a function that executes an sql command and prints an error if it fails, indicating the row in which the error occured.\n",
    "\n",
    "We will also use this function for the reports in problem 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requires to be connected to the database examweek2\n",
    "\n",
    "### Creating a function in order to avoid repetition\n",
    "def sql_command(connection, sql_command, error_id_value = \"default\", error_id_field = \"default\"):\n",
    "    \"\"\"\n",
    "    Executes an sql command and returns the result if it is successful. \n",
    "    Prints an error if it fails. If applicable (i. e. when inserting data),\n",
    "    the error message also contains a field and value of the entry in which the error occured.\n",
    "    \n",
    "    Parameters:\n",
    "    connection: an open pymysql connection to the database.\n",
    "    sql_command: the sql command to be executed.\n",
    "    error_id_value: (optional) value of the entry which created an error, if applicable.\n",
    "    error_id_field: (optional) field of the entry which created an error, if applicable.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with connection.cursor() as cursor:\n",
    "            cursor.execute(sql_command) # executes the command\n",
    "            results = cursor.fetchall() # gets the results\n",
    "            return results # returns the results\n",
    "    except Exception as err: # stores any possible exception in err\n",
    "        if not error_id_value == error_id_field == \"default\": \n",
    "            # we only want to print the row if those parameters aren't default\n",
    "            print(\"There was an error in the row that contains '{}' in field '{}'\".format(error_id_value, error_id_field))\n",
    "        print(\"Error: \", err) # outside of the if, so it always prints the exception"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the main part of the code, we open both .tsv files, create a csv.DictReader for each of them (the same as in problem 1).\n",
    "\n",
    "We iterate through each element of both DictReaders using a zip. Each element is a dictionary, so the values are accesed using its keys (the header names of the files).\n",
    "\n",
    "* Note that, if there were a different number of entries in each .tsv file, zip stops when the smallest one ends, so some entries would not get inserted into the database. It is not the case with the current .tsv files, but if that was the case, we could iterate through each DictReader separately.\n",
    "\n",
    "Inside of the loop, we first create the sql command, (making sure that the varchar(x) are surrounded by single quotes). Then we use the function I created to try to insert the values of each element into the database using pymysql. If it gives an error for any element, the except clause will print an error message indicating the row in which it occured.\n",
    " \n",
    " * In fact, the error message that I added to the except clause was useful, because I originally had made 'germplasm' in the germplasm table a VARCHAR(20), and it made me realize that the germplasm field for locus AT3G02260 (tir3-1 RGLG1:rglg1 rglg2) was longer than that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Main part of the code:\n",
    "\n",
    "# Opening the .tsv files\n",
    "filename1 = \"LocusGene.tsv\"\n",
    "filename2 = \"Germplasm.tsv\"\n",
    "\n",
    "with open(filename1) as file1, open(filename2) as file2:\n",
    "    # opening each file with csv.DictReader\n",
    "    locusgene = csv.DictReader(file1, delimiter=\"\\t\", quotechar='\"') # default fieldnames because of the header\n",
    "    germplasm = csv.DictReader(file2, delimiter=\"\\t\", quotechar='\"') # default fieldnames because of the header\n",
    "    \n",
    "    # iterating through each element of both DictReaders:\n",
    "    for locusrow, germrow in zip(locusgene, germplasm):\n",
    "        \n",
    "        # inserting locusgene entry into the database:\n",
    "        sql = \"\"\"INSERT INTO locusgene (locus, gene, protein_length) \n",
    "                 VALUES ('\"\"\" + locusrow[\"Locus\"] + \"\"\"', '\"\"\" + locusrow[\"Gene\"] + \"\"\"', \n",
    "                 \"\"\" + locusrow[\"ProteinLength\"] + \"\"\" )\"\"\"\n",
    "        \n",
    "        sql_command(connection, sql, error_id_value = locusrow[\"Locus\"], error_id_field = \"Locus\")\n",
    "        # We don't need to store ids because there isn't any auto incremented id in my database design\n",
    "        \n",
    "        # inserting germplasm entry into the database:\n",
    "        sql = \"\"\"INSERT INTO germplasm (locus, germplasm, phenotype, pubmed) \n",
    "                 VALUES ('\"\"\" + germrow[\"Locus\"] + \"\"\"', '\"\"\" + germrow[\"germplasm\"] + \"\"\"', \n",
    "                 '\"\"\" + germrow[\"phenotype\"] + \"\"\"', \"\"\" + germrow[\"pubmed\"] + \"\"\" )\"\"\" \n",
    "\n",
    "        sql_command(connection, sql, error_id_value = germrow[\"Locus\"], error_id_field = \"Locus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%sql SELECT * FROM germplasm #to check if everything works correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%sql SELECT * FROM locusgene #to check if everything works correctly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4: Create reports, written to a file\n",
    "\n",
    "1. Create a report that shows the full, joined, content of the two database tables (including a header line)\n",
    "\n",
    "2. Create a joined report that only includes the Genes SKOR and MAA3\n",
    "\n",
    "3. Create a report that counts the number of entries for each Chromosome (AT1Gxxxxxx to AT5Gxxxxxxx)\n",
    "\n",
    "4. Create a report that shows the average protein length for the genes on each Chromosome (AT1Gxxxxxx to AT5Gxxxxxxx)\n",
    "\n",
    "When creating reports 2 and 3, remember the \"Don't Repeat Yourself\" rule! \n",
    "\n",
    "All reports should be written to **the same file**.  You may name the file anything you wish.\n",
    "\n",
    "------\n",
    "\n",
    "### Explanation problem 4:\n",
    "\n",
    "First we would connect to the database examweek2 with pymysql like we did in problem 3, but we already did it and I haven't closed the connection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requires to be connected to the database examweek2 (should already be connected from problem 3)\n",
    "#connection = pymysql.connect(host='localhost',\n",
    "#                             user='root',\n",
    "#                             password='root',\n",
    "#                             db='examweek2', # database name\n",
    "#                             charset='utf8mb4',  \n",
    "#                             cursorclass=pymysql.cursors.DictCursor,\n",
    "#                             autocommit = True) # I'm setting autocommit to True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Opening a file in \"a\" mode already creates a new one if it didn't exist, however I wanted to make sure that the new file is empty. Therefore I first open it in \"w\" mode (overwrite) and close it immediately. The reports will get appended (\"a\") to this empty file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new file (overwriting the existing one if any exists with that name):\n",
    "output_f = open(\"output_exam2.tsv\", \"w\")\n",
    "output_f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reports 1 and 2:\n",
    "\n",
    "Because the sql command is the only difference between the two reports, I am joining their explanation. For the sql command, I am using the function I created for problem 3 (sql_command).\n",
    "\n",
    "Following the usual pymysql syntax, I first execute the sql_command. It is a SELECT query, selecting everything from an inner join between the two tables, linking them via their locus field. I fetch the results (a list of dictionaries) and store them.\n",
    "\n",
    "- In the case of report 1, there are no additional clauses.\n",
    "- In report 2, I include a WHERE clause in the sql command, so that the results only include the genes SKOR and MAA3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Report 1:\n",
    "sql1 = \"\"\"SELECT * FROM locusgene INNER JOIN germplasm ON germplasm.locus = locusgene.locus;\"\"\"\n",
    "# This command performs an inner join to read everything from both tables. \n",
    "# In this case, any join would be equivalent because every locus exists \n",
    "# in both tables and there aren't any more tables.\n",
    "results1 = sql_command(connection, sql1) # executing the command with my function\n",
    "# in these cases, I am not inserting data, so I won't use the additional arguments.\n",
    "# if there was an error, it would only print the error, not the row where it happened\n",
    "# (it wouldn't make sense in this situation).\n",
    "\n",
    "\n",
    "### Report 2:\n",
    "sql2 = \"\"\"SELECT * FROM locusgene INNER JOIN germplasm \n",
    "          ON germplasm.locus = locusgene.locus \n",
    "          WHERE locusgene.gene = 'SKOR' OR locusgene.gene = 'MAA3' ;\"\"\"\n",
    "results2 = sql_command(connection, sql2) # executing the command with my function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For writing the report into the file, I also created a function.\n",
    "\n",
    "I will use csv.DictWriter because it is easy and efficient when we are working with dictionaries or lists of dictionaries. In order to use this, I need the headers of the file, which I get from one of the dictionaries in results.\n",
    "\n",
    "Opening the file in append mode, I write normally Report X:, and then create the csv.DictWriter. I use the headers as fieldnames and \"\\t\" as a delimiter in order to make a .tsv file.\n",
    "\n",
    "   * (It is not a 100% tsv file, because of the line \"Report X: \\n\", but I wanted to keep it organised because every report is in the same file).\n",
    "   \n",
    "Using the DictWriter, writeheader() writes the header onto the file, and writerows() takes a list of dictionaries and writes the information into the file in the correct fields (columns), each dictionary being one line.\n",
    "\n",
    "Finally I write a newline in order to separate the reports.\n",
    "\n",
    "   * I am using writerows(), which gives an error if you only input one dictionary. However, inputing a list of one dictionary works, so we have to be careful when calling this function (in reports 3 and 4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating a function to write reports to a file\n",
    "def write_report_to_file(filename, list_of_dicts, mode = \"a\", title = \"\", delimiter = \"\\t\", ending = \"\\n\"):\n",
    "    \"\"\"\n",
    "    Writes a report into a file\n",
    "    \n",
    "    Parameters:\n",
    "    - filename: name of the file\n",
    "    - list_of_dicts: list of dictionaries, MUST BE A LIST. \n",
    "      If it is only 1 dictionary, append it to an empty list or do [dictionary].\n",
    "    - mode: (optional) default \"a\" append, could also use \"w\" overwrite\n",
    "    - title: (optional) default \"\" (no title), title of the report\n",
    "    - delimiter: (optional) default \"\\t\", character that separates the elements of a row.\n",
    "    - ending: (optional) default \"\\n\", ending of the report.\n",
    "    \n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(filename, mode) as output_f:\n",
    "            output_f.write(title) # title of the report\n",
    "            header = list_of_dicts[0].keys() # getting the header from the first dictionary of the list\n",
    "            reportwriter = csv.DictWriter(output_f, fieldnames = header, delimiter = delimiter)\n",
    "            reportwriter.writeheader() # writing the header\n",
    "            reportwriter.writerows(list_of_dicts) # writing the results into the file, writerows needs a list of dictionaries.\n",
    "            output_f.write(ending) # separate the reports, maybe\n",
    "    except Exception as err: # prints error message if any\n",
    "        print(err)\n",
    "\n",
    "## Writing reports 1 and 2\n",
    "write_report_to_file(filename = \"output_exam2.tsv\", list_of_dicts = results1, title = \"Report 1: \\n\")\n",
    "write_report_to_file(filename = \"output_exam2.tsv\", list_of_dicts = results2, title = \"Report 2: \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Report 3:\n",
    "In this case, because the sql command is more complex, I tried to make it as general as possible. That's why the fieldname, tablename and regex are variables, so they can easily be changed.\n",
    "\n",
    "I create a list of the five regular expressions using a list comprehension because they only differ in the chromosome number.\n",
    "\n",
    "I then create an empty dictionary in which I will store the results, with keys \"Chromosome \" and the number, and values the number of entries of that chromosome in the table.\n",
    "\n",
    "Then, for each regular expression in the list I created, I create the sql command. I then execute it and store the results using my function sql_command from problem 3, extract the number of entries and enter it into my num_of_entries dictionary.\n",
    "\n",
    "I then write the report into the file. Because my function requires a list of dictionaries, and num_of_entries is only one dictionary, I pass it to the function inside of a list.\n",
    "\n",
    "##### The sql command in more detail:\n",
    "For example, for the first chromosome, the command would be:\n",
    "\n",
    "SELECT COUNT(*) AS 'Number of matches' FROM locusgene WHERE locus REGEXP 'AT1G[0-9]{5}';\n",
    "\n",
    "It will count the number of entries in the table locusgene where the field locus matches the regular expression 'AT1G[0-9]{5}', and output a list of one dictionary whose key is 'Number of matches'.\n",
    "\n",
    "However, by not hardcoding the command, we could look for other regular expressions in other fields easily by changing the appropriate variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Report 3:\n",
    "fieldname = \"locus\" # name of the field (column) of the table\n",
    "tablename = \"locusgene\" # name of the table\n",
    "\n",
    "# creates a list of the corresponding regex for the chromosomes 1 to 5 (0 to 4, +1)\n",
    "chromosome_regexs = [\"AT\" + str(num + 1) + \"G[0-9]{5}\" for num in range(5)]\n",
    "\n",
    "num_of_entries = {} # \"Chromosome \" + number : number of entries\n",
    "# will store the results\n",
    "\n",
    "for regex in chromosome_regexs: # iterating through the list of regex\n",
    "    \n",
    "    ### Creating the command:\n",
    "    sql3 = \"\"\"SELECT COUNT(*) AS 'Number of matches' FROM \"\"\" + tablename + \"\"\" \n",
    "                     WHERE \"\"\" + fieldname + \"\"\" REGEXP '\"\"\" + regex + \"\"\"'; \"\"\"\n",
    "    \n",
    "    ### Executing the command and storing the numeric result into a dictionary:\n",
    "    results3 = sql_command(connection, sql3) # executing the command with my function\n",
    "    count = results3[0][\"Number of matches\"] # getting the number from results3\n",
    "    num_of_entries[\"Chromosome \" + regex[2]] = count # index 2 of the regex is the chromosome number\n",
    "\n",
    "## Writing the report 3:\n",
    "write_report_to_file(filename = \"output_exam2.tsv\", list_of_dicts = [num_of_entries], title = \"Report 3: \\n\")\n",
    "# I input the dictionary inside of a list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Report 4:\n",
    "\n",
    "Same idea as report 3, but with different variable names.\n",
    "\n",
    "The sql command in this case returns the average of every field 'protein_length' from the table 'locusgene' where the field 'locus' matches the regular expression of each chromosome.\n",
    "\n",
    "For example, for the first chromosome, the command would be:\n",
    "\n",
    "SELECT AVG(protein_length) AS 'Average' FROM locusgene WHERE locus REGEXP 'AT1G[0-9]{5}';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Report 4:\n",
    "mean_fieldname = \"protein_length\" # name of the field where the mean is going to be computed\n",
    "regex_fieldname = \"locus\" # name of the field where the regular expression is going to be matched\n",
    "tablename = \"locusgene\" # name of the table\n",
    "\n",
    "# we already have the chromosome_regexs list from report 3\n",
    "#chromosome_regexs = [\"AT\" + str(num + 1) + \"G[0-9]{5}\" for num in range(5)]\n",
    "\n",
    "average_lengths = {} # \"Chromosome \" + number : average protein length\n",
    "# will store the results\n",
    "\n",
    "for regex in chromosome_regexs:\n",
    "    ### Creating the command:\n",
    "    sql4 = \"\"\"SELECT AVG(\"\"\" + mean_fieldname + \"\"\") AS 'Average' FROM \"\"\" + tablename + \"\"\" \n",
    "              WHERE \"\"\" + regex_fieldname + \"\"\" REGEXP '\"\"\" + regex + \"\"\"'; \"\"\"\n",
    "    \n",
    "    ### Executing the command and storing the numeric result into a dictionary:\n",
    "    results4 = sql_command(connection, sql4) # executing the command with my function\n",
    "    average = results4[0][\"Average\"] # getting the number from results4\n",
    "    average_lengths[\"Chromosome \" + regex[2]] = average # index 2 of the regex is the chromosome number\n",
    "\n",
    "## Writing the report 4:\n",
    "write_report_to_file(filename = \"output_exam2.tsv\", list_of_dicts = [average_lengths], title = \"Report 4: \\n\")\n",
    "# I input the dictionary inside of a list."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
