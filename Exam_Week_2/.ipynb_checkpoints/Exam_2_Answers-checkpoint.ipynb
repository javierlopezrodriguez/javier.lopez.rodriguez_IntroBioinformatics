{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exam 2 Answers by Javier López Rodríguez  (javier.lopez.rodriguez@alumnos.upm.es)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1:  Controls\n",
    "\n",
    "Write a Python script that proves that the lines of data in Germplasm.tsv, and LocusGene are in the same sequence, based on the AGI Locus Code (ATxGxxxxxx).  (hint: This will help you decide how to load the data into the database)\n",
    "\n",
    "---------\n",
    "\n",
    "#### Explanation for problem 1:\n",
    "\n",
    "We are dealing with .tsv files with headers. In order to read them, I'll use csv.DictReader because it is more efficient and easier (and I haven't used it before this course, unlike the normal python input/output, so it helps me practice new things).\n",
    "\n",
    "After opening both of the files, I first iterate through each file one time because I haven't found a method of csv.DictReader that returns its length. The object csv.DictReader is an iterator, so it doesn't load every line to memory at the same time, unlike a list. Therefore, converting it into a list in order to use len() is a memory dangerous process, so I avoided doing that.\n",
    "\n",
    "Then, I reset the pointer to the start and create the csv.DictReader objects for both files.\n",
    "\n",
    "Because we want to prove that both files have the same locus code in the same line, the number of lines should be the same in both cases. I check this (that's why I need to iterate the first time for the lengths). If the lengths aren't the same, it will only compare until the smallest file ends (minimum of length1 and length2).\n",
    "\n",
    "I create two lists for storing the line number of matches and mismatches (to check at the end if there is any mismatch, how many there are, and if the number of matches and mismatches is what we expected).\n",
    "\n",
    "Then, in a for loop, I iterate through both DictReaders at the same time, using the zip function. In case of different number of entries, zip stops when the smallest one ends (this is equivalent to iterating until the minimum of length1 and length2).\n",
    "\n",
    "I go through every pair of entries, store the Locus code (which DictReader makes very easy because each row is a dictionary and there's no need to split/do anything else), and compare both codes. I append the line number to either list, depending on it being a match or a mismatch.\n",
    "\n",
    " * Note that zip creates an iterator, not a list. Therefore, a zip of two DictReaders still does not load everything into memory at the same time, so the advantage of using DictReaders is maintained.\n",
    "\n",
    "In the end, I check if there are any mismatches (and print them), print the number of matches, and have a third condition just in case the number of matches + mismatches is less than the number of compared lines. The latter is just a precaution because (I think) it should never happen unless the code is incorrect.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Both files have the same number of lines: 32 (without header).\n",
      "No mismatches found. There were 32 lines with matching Locus code.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "filename1 = \"LocusGene.tsv\"\n",
    "filename2 = \"Germplasm.tsv\"\n",
    "\n",
    "with open(filename1) as file1, open(filename2) as file2:\n",
    "        \n",
    "    # getting the length of each file iterating through each line and adding 1 for each line\n",
    "    # substracting 1 so that we don't count the header\n",
    "    length1 = sum(1 for _ in file1) - 1 \n",
    "    length2 = sum(1 for _ in file2) - 1 \n",
    "\n",
    "    # reset the pointer to the start of each file\n",
    "    file1.seek(0) \n",
    "    file2.seek(0)\n",
    "\n",
    "    # opening each file with csv.DictReader\n",
    "    locusgene = csv.DictReader(file1, delimiter=\"\\t\", quotechar='\"') # default fieldnames because of the header\n",
    "    germplasm = csv.DictReader(file2, delimiter=\"\\t\", quotechar='\"') # default fieldnames because of the header\n",
    "\n",
    "    if length1 != length2: # different number of lines\n",
    "        print(\"Warning: There are not the same number of lines in both files ({} and {})\".format(length1, length2))\n",
    "        print(\"Only the first {} lines of each file will be compared.\".format(min(length1, length2)))\n",
    "    else: # equal number of lines\n",
    "        print(\"Both files have the same number of lines: {} (without header).\".format(length1))\n",
    "\n",
    "    mismatched_lines = [] # will store the indexes of the mismatched lines, if any\n",
    "    correct_lines = [] # will store the indexes of the correct lines\n",
    "\n",
    "    # iterating through every pair of elements\n",
    "    linenumber = 1 # keeps track of the line number we're in, starts in 1 (because we skip the header using DictReader)\n",
    "    for entry1, entry2 in zip(locusgene, germplasm): # iterating through both DictReaders at the same time\n",
    "        locus1, locus2 = entry1[\"Locus\"], entry2[\"Locus\"]\n",
    "        #print(locus1 + \" \" + locus2) # checking that locus1 and locus2 contain the expected strings\n",
    "        # checking if they match or mismatch\n",
    "        if locus1 == locus2: # match\n",
    "            correct_lines.append(linenumber)\n",
    "        else: # mismatch\n",
    "            mismatched_lines.append(linenumber)\n",
    "        linenumber += 1 # increment linenumber\n",
    "        \n",
    "    if len(mismatched_lines) > 0: # there are mismatches, output them\n",
    "        print(\"Warning: There are some mismatches.\")\n",
    "        print(\"Mismatched lines: \" + \" \".join(mismatched_lines))\n",
    "        print(\"There were {} lines with matching Locus code.\".format(len(correct_lines)))\n",
    "    elif len(correct_lines) == min(length1, length2): # there are no mismatches and every line checked was a match\n",
    "        print(\"No mismatches found. There were {} lines with matching Locus code.\".format(len(correct_lines)))\n",
    "    else: # there are no mismatches but not every line checked was a match -> this should never happen\n",
    "        print(\"Error: there were less matches than expected. Something went wrong.\")\n",
    "\n",
    "# using \"with open(...) as ...\", we don't need to close the files afterwards, it is done automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alternative solution for problem 1:\n",
    "This problem could also be solved using the pandas library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of items in LocusGene.tsv is 32\n",
      "Number of items in Germplasm.tsv is 32\n",
      "Number of matching Locus codes: 32\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "filename1 = \"LocusGene.tsv\"\n",
    "filename2 = \"Germplasm.tsv\"\n",
    "\n",
    "# reading the .tsv into pandas dataframes\n",
    "df1 = pd.read_csv(filename1, sep = \"\\t\")\n",
    "df2 = pd.read_csv(filename2, sep = \"\\t\")\n",
    "\n",
    "# renaming the columns so that, when concatenating the columns, they are named differently\n",
    "df1 = df1.rename(columns = {\"Locus\": \"Locus1\"})\n",
    "df2 = df2.rename(columns = {\"Locus\": \"Locus2\"})\n",
    "\n",
    "# printing number of items (size) of each column\n",
    "print(\"Number of items in \" + filename1 + \" is {}\".format(df1[\"Locus1\"].size))\n",
    "print(\"Number of items in \" + filename2 + \" is {}\".format(df2[\"Locus2\"].size))\n",
    "\n",
    "# concatenating both columns so that the following comparison can be made\n",
    "# if the number of elements is different, pd.concat adds NaN to the missing elements of the smallest column\n",
    "# so that both columns have the same length\n",
    "dfconcat = pd.concat([df1[\"Locus1\"], df2[\"Locus2\"]], axis = 1)\n",
    "\n",
    "# comparing the contents of both columns\n",
    "# the comparison gives a boolean array, the sum of that array is the number of True elements (number of matches)\n",
    "# doing this without concatenating both columns first gives an error if the number of elements is different,\n",
    "# that is why we need to concatenate both columns first into the same data frame\n",
    "print(\"Number of matching Locus codes: \" + str(sum(dfconcat[\"Locus1\"] == dfconcat[\"Locus2\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2:  Design and create the database\n",
    "* It should have two tables - one for each of the two data files.\n",
    "* The two tables should be linked in a 1:1 relationship\n",
    "* you may use either sqlMagic or pymysql to build the database\n",
    "\n",
    "---------\n",
    "\n",
    "#### Explanation for problem 2:\n",
    "\n",
    "I'm using sqlMagic because I find it easier for creating databases and tables.\n",
    "\n",
    "We know that both files contain the same AGI Locus codes in the same positions, and both tables are going to have that field. \n",
    "\n",
    "Because the relationship between the two tables is 1:1 and the AGI Locus code in this case is a unique identifier of each entry in both tables, I am going to use it as the primary key of both tables. Therefore, the tables won't include additional numeric ids. Linking one table with the other in queries that involve both is going to happen via the AGI Locus codes.\n",
    "\n",
    "When creating the table locusgene:\n",
    "\n",
    "    'locus' is the primary key, a not null string of at most 10 characters,\n",
    "    'gene' is a not null string of at most 10 characters,\n",
    "    'protein_length' is a not null integer.\n",
    "    \n",
    "When creating the table germplasm:\n",
    "\n",
    "    'locus' is the primary key, a not null string of at most 10 characters (and the same as in the above table),\n",
    "    'germplasm' is a not null string of at most 50 characters,\n",
    "    'phenotype' is a not null string of at most 500 characters,\n",
    "    'pubmed' is a not null integer.\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Connected: root@mysql'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Connecting to sqlMagic\n",
    "%load_ext sql\n",
    "#%config SqlMagic.autocommit=False\n",
    "%sql mysql+pymysql://root:root@127.0.0.1:3306/mysql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * mysql+pymysql://root:***@127.0.0.1:3306/mysql\n",
      "2 rows affected.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#%sql drop database examweek2; # dropping the database in case it already exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * mysql+pymysql://root:***@127.0.0.1:3306/mysql\n",
      "1 rows affected.\n",
      " * mysql+pymysql://root:***@127.0.0.1:3306/mysql\n",
      "0 rows affected.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating the database examweek2:\n",
    "%sql create database examweek2;\n",
    "# In order to interact with the database:\n",
    "%sql use examweek2;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * mysql+pymysql://root:***@127.0.0.1:3306/mysql\n",
      "0 rows affected.\n",
      " * mysql+pymysql://root:***@127.0.0.1:3306/mysql\n",
      "0 rows affected.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating the table locusgene\n",
    "%sql CREATE TABLE locusgene (locus VARCHAR(10) NOT NULL PRIMARY KEY, \\\n",
    "                             gene VARCHAR(10) NOT NULL, \\\n",
    "                             protein_length INTEGER NOT NULL);\n",
    "# Creating the table germplasm\n",
    "%sql CREATE TABLE germplasm (locus VARCHAR(10) NOT NULL PRIMARY KEY, \\\n",
    "                             germplasm VARCHAR(50) NOT NULL, \\\n",
    "                             phenotype VARCHAR(500) NOT NULL, \\\n",
    "                             pubmed INTEGER NOT NULL);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * mysql+pymysql://root:***@127.0.0.1:3306/mysql\n",
      "3 rows affected.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>Field</th>\n",
       "        <th>Type</th>\n",
       "        <th>Null</th>\n",
       "        <th>Key</th>\n",
       "        <th>Default</th>\n",
       "        <th>Extra</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>locus</td>\n",
       "        <td>varchar(10)</td>\n",
       "        <td>NO</td>\n",
       "        <td>PRI</td>\n",
       "        <td>None</td>\n",
       "        <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>gene</td>\n",
       "        <td>varchar(10)</td>\n",
       "        <td>NO</td>\n",
       "        <td></td>\n",
       "        <td>None</td>\n",
       "        <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>protein_length</td>\n",
       "        <td>int(11)</td>\n",
       "        <td>NO</td>\n",
       "        <td></td>\n",
       "        <td>None</td>\n",
       "        <td></td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[('locus', 'varchar(10)', 'NO', 'PRI', None, ''),\n",
       " ('gene', 'varchar(10)', 'NO', '', None, ''),\n",
       " ('protein_length', 'int(11)', 'NO', '', None, '')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%sql DESCRIBE locusgene; # Describing the tables to check that they are correctly made"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * mysql+pymysql://root:***@127.0.0.1:3306/mysql\n",
      "4 rows affected.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>Field</th>\n",
       "        <th>Type</th>\n",
       "        <th>Null</th>\n",
       "        <th>Key</th>\n",
       "        <th>Default</th>\n",
       "        <th>Extra</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>locus</td>\n",
       "        <td>varchar(10)</td>\n",
       "        <td>NO</td>\n",
       "        <td>PRI</td>\n",
       "        <td>None</td>\n",
       "        <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>germplasm</td>\n",
       "        <td>varchar(50)</td>\n",
       "        <td>NO</td>\n",
       "        <td></td>\n",
       "        <td>None</td>\n",
       "        <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>phenotype</td>\n",
       "        <td>varchar(500)</td>\n",
       "        <td>NO</td>\n",
       "        <td></td>\n",
       "        <td>None</td>\n",
       "        <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>pubmed</td>\n",
       "        <td>int(11)</td>\n",
       "        <td>NO</td>\n",
       "        <td></td>\n",
       "        <td>None</td>\n",
       "        <td></td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[('locus', 'varchar(10)', 'NO', 'PRI', None, ''),\n",
       " ('germplasm', 'varchar(50)', 'NO', '', None, ''),\n",
       " ('phenotype', 'varchar(500)', 'NO', '', None, ''),\n",
       " ('pubmed', 'int(11)', 'NO', '', None, '')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%sql DESCRIBE germplasm; # Describing the tables to check that they are correctly made"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3: Fill the database\n",
    "Using pymysql, create a Python script that reads the data from these files, and fills the database.  There are a variety of strategies to accomplish this.  I will give all strategies equal credit - do whichever one you are most confident with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation for problem 3:\n",
    "\n",
    "First, we import pymysql.cursors and connect to the examweek2 database, making sure that the character set is UTF8 and that autocommit is set to True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing pymysql.cursors and connecting to the database\n",
    "import pymysql.cursors\n",
    "\n",
    "# Connecting to the database examweek2\n",
    "connection = pymysql.connect(host='localhost',\n",
    "                             user='root',\n",
    "                             password='root',\n",
    "                             db='examweek2', # database name\n",
    "                             charset='utf8mb4',  \n",
    "                             cursorclass=pymysql.cursors.DictCursor,\n",
    "                             autocommit = True) # I'm setting autocommit to True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the design I've chosen for the database, because the relationship is 1:1 and both of them have the same primary key and no additional id, it doesn't matter which table we fill out first. We don't need to keep track of the last inserted id (because there is none).\n",
    "\n",
    "In order to avoid repetition, I first created a function that executes an sql command and prints an error if it fails, indicating the row in which the error occured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requires to be connected to the database examweek2\n",
    "\n",
    "### Creating a function in order to avoid repetition\n",
    "def input_row_into_sql(connection, sql_command, error_id_value = \"default\", error_id_field = \"default\"):\n",
    "    \"\"\"\n",
    "    Executes an sql command and prints an error if it fails. If applicable (i. e. when inserting data),\n",
    "    the error message also contains a field and value of the entry in which the error occured.\n",
    "    \n",
    "    Parameters:\n",
    "    connection: an open pymysql connection to the database.\n",
    "    sql_command: the sql command to be executed.\n",
    "    error_id_value: (optional) value of the entry which created an error, if applicable.\n",
    "    error_id_field: (optional) field of the entry which created an error, if applicable.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with connection.cursor() as cursor:\n",
    "            cursor.execute(sql_command) # executes the command\n",
    "    except Exception as err: # stores any possible exception in err\n",
    "        if not error_id_value == error_id_field == \"default\": \n",
    "            # we only want to print the row if those parameters aren't default\n",
    "            print(\"There was an error in the row that contains '{}' in field '{}'\".format(error_id_value, error_id_field))\n",
    "        print(\"Error: \", err) # outside of the if, so it always prints the exception"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the main part of the code, we open both .tsv files, create a csv.DictReader for each of them (the same as in problem 1).\n",
    "\n",
    "We iterate through each element of both DictReaders using a zip. Each element is a dictionary, so the values are accesed using its keys (the header names of the files).\n",
    "\n",
    "* Note that, if there were a different number of entries in each .tsv file, zip stops when the smallest one ends, so some entries would not get inserted into the database. It is not the case with the current .tsv files, but if that was the case, we could iterate through each DictReader separately.\n",
    "\n",
    "Inside of the loop, we first create the sql command, (making sure that the varchar(x) are surrounded by single quotes). Then we use the function I created to try to insert the values of each element into the database using pymysql. If it gives an error for any element, the except clause will print an error message indicating the row in which it occured.\n",
    " \n",
    " * In fact, the error message that I added to the except clause was useful, because I originally had made 'germplasm' in the germplasm table a VARCHAR(20), and it made me realize that the germplasm field for locus AT3G02260 (tir3-1 RGLG1:rglg1 rglg2) was longer than that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Main part of the code:\n",
    "\n",
    "# Opening the .tsv files\n",
    "filename1 = \"LocusGene.tsv\"\n",
    "filename2 = \"Germplasm.tsv\"\n",
    "\n",
    "with open(filename1) as file1, open(filename2) as file2:\n",
    "    # opening each file with csv.DictReader\n",
    "    locusgene = csv.DictReader(file1, delimiter=\"\\t\", quotechar='\"') # default fieldnames because of the header\n",
    "    germplasm = csv.DictReader(file2, delimiter=\"\\t\", quotechar='\"') # default fieldnames because of the header\n",
    "    \n",
    "    # iterating through each element of both DictReaders:\n",
    "    for locusrow, germrow in zip(locusgene, germplasm):\n",
    "        \n",
    "        # inserting locusgene entry into the database:\n",
    "        sql = \"\"\"INSERT INTO locusgene (locus, gene, protein_length) \n",
    "                 VALUES ('\"\"\" + locusrow[\"Locus\"] + \"\"\"', '\"\"\" + locusrow[\"Gene\"] + \"\"\"', \n",
    "                 \"\"\" + locusrow[\"ProteinLength\"] + \"\"\" )\"\"\"\n",
    "        \n",
    "        input_row_into_sql(connection, sql, error_id_value = locusrow[\"Locus\"], error_id_field = \"Locus\")\n",
    "        # We don't need to store ids because there isn't any auto incremented id in my database design\n",
    "        \n",
    "        # inserting germplasm entry into the database:\n",
    "        sql = \"\"\"INSERT INTO germplasm (locus, germplasm, phenotype, pubmed) \n",
    "                 VALUES ('\"\"\" + germrow[\"Locus\"] + \"\"\"', '\"\"\" + germrow[\"germplasm\"] + \"\"\"', \n",
    "                 '\"\"\" + germrow[\"phenotype\"] + \"\"\"', \"\"\" + germrow[\"pubmed\"] + \"\"\" )\"\"\" \n",
    "\n",
    "        input_row_into_sql(connection, sql, error_id_value = germrow[\"Locus\"], error_id_field = \"Locus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%sql SELECT * FROM germplasm; #to check if everything works correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%sql SELECT * FROM locusgene; #to check if everything works correctly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4: Create reports, written to a file\n",
    "\n",
    "1. Create a report that shows the full, joined, content of the two database tables (including a header line)\n",
    "\n",
    "2. Create a joined report that only includes the Genes SKOR and MAA3\n",
    "\n",
    "3. Create a report that counts the number of entries for each Chromosome (AT1Gxxxxxx to AT5Gxxxxxxx)\n",
    "\n",
    "4. Create a report that shows the average protein length for the genes on each Chromosome (AT1Gxxxxxx to AT5Gxxxxxxx)\n",
    "\n",
    "When creating reports 2 and 3, remember the \"Don't Repeat Yourself\" rule! \n",
    "\n",
    "All reports should be written to **the same file**.  You may name the file anything you wish.\n",
    "\n",
    "---------\n",
    "\n",
    "# Explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requires to be connected to the database examweek2 (should already be connected from problem 3)\n",
    "#connection = pymysql.connect(host='localhost',\n",
    "#                             user='root',\n",
    "#                             password='root',\n",
    "#                             db='examweek2', # database name\n",
    "#                             charset='utf8mb4',  \n",
    "#                             cursorclass=pymysql.cursors.DictCursor,\n",
    "#                             autocommit = True) # I'm setting autocommit to True\n",
    "\n",
    "\n",
    "\n",
    "### Report 1\n",
    "try:\n",
    "    with connection.cursor() as cursor:\n",
    "        # Performs a full outer join to read everything from both tables. \n",
    "        # In this case, any join would be equivalent because every locus exists in both tables. \n",
    "        sql = \"\"\"SELECT * FROM locusgene FULL OUTER JOIN germplasm \n",
    "                 ON germplasm.locus = locusgene.locus;\"\"\"\n",
    "        cursor.execute(sql)\n",
    "        results = cursor.fetchall()\n",
    "        for result in results:\n",
    "            print(result)\n",
    "            print()\n",
    "except Exception as err:\n",
    "    print(err)\n",
    "\n",
    "### Report 2\n",
    "try:\n",
    "    with connection.cursor() as cursor:\n",
    "        # Performs a full outer join to read everything from both tables. \n",
    "        # In this case, any join would be equivalent because every locus exists in both tables. \n",
    "        sql = \"\"\"SELECT * FROM locusgene FULL OUTER JOIN germplasm \n",
    "                 ON germplasm.locus = locusgene.locus \n",
    "                 WHERE locusgene.gene = 'SKOR' OR locusgene.gene = 'MAA3' ;\"\"\"\n",
    "        cursor.execute(sql)\n",
    "        results = cursor.fetchall()\n",
    "        for result in results:\n",
    "            print(result)\n",
    "            print()\n",
    "except Exception as err:\n",
    "    print(err)\n",
    "\n",
    "### Report 3\n",
    "## Creating a function:\n",
    "def count_regex_in_field(regex, fieldname, tablename):\n",
    "    \"\"\"\n",
    "    Counts the number of entries of a table that match a regular expression in one of its fields.\n",
    "    \n",
    "    Requires an open pymysql connection to the database.\n",
    "    \n",
    "    Parameters:\n",
    "    regex: regular expression (in sql format)\n",
    "    fieldname: the name of the field (column) of the table\n",
    "    tablename: the name of the table\n",
    "    \n",
    "    Returns: the number of entries, or None if there was an error.\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        with connection.cursor() as cursor:\n",
    "            sql = \"\"\"SELECT COUNT(*) AS 'Number of matches' FROM \"\"\" + tablename + \"\"\" \n",
    "                     WHERE \"\"\" + fieldname + \"\"\" REGEXP '\"\"\" + regex + \"\"\"'; \"\"\"\n",
    "            cursor.execute(sql)\n",
    "            results = cursor.fetchall()\n",
    "            count = results[0][\"Number of matches\"]\n",
    "    except Exception as err:\n",
    "        print(err)\n",
    "        count = None\n",
    "    return count\n",
    "\n",
    "## Generating the report:\n",
    "# creates a list of the corresponding regex for the chromosomes 1 to 5 (0 to 4, +1)\n",
    "chromosome_regexs = [\"AT\" + str(num + 1) + \"G[0-9]{5}\" for num in range(5)]\n",
    "\n",
    "num_of_entries = {} # chromosome number : number of entries\n",
    "for chr_regex in chromosome_regexs:\n",
    "    count = count_regex_in_field(regex = chr_regex, fieldname = \"locus\", tablename = \"locusgene\")\n",
    "    num_of_entries[chr_regex[2]] = count # index 2 of the regex is the chromosome number\n",
    "\n",
    "### Report 4\n",
    "## Creating a function:\n",
    "def mean_of_field_where_regex_in_field(mean_fieldname, regex, regex_fieldname, tablename):\n",
    "    \"\"\"\n",
    "    Given a table, calculates the mean of the elements of a field \n",
    "    of every entry in which another field matches a regular expression.\n",
    "    \n",
    "    Requires an open pymysql connection to the database.\n",
    "    \n",
    "    Parameters:\n",
    "    mean_fieldname: the name of the field where the mean is going to be computed\n",
    "    regex: the regular expression to match on field regex_fieldname\n",
    "    regex_fieldname: the name of the field where the regular expression is going to be matched\n",
    "    tablename: the name of the table\n",
    "    \n",
    "    Returns: the mean of the elements, or None if there was an error.\n",
    "    \"\"\"\n",
    "    # requires an open connection to the database\n",
    "    try:\n",
    "        with connection.cursor() as cursor:\n",
    "            sql = \"\"\"SELECT AVG(\"\"\" + mean_fieldname + \"\"\") AS 'Average' FROM \"\"\" + tablename + \"\"\" \n",
    "                     WHERE \"\"\" + fieldname + \"\"\" REGEXP '\"\"\" + regex + \"\"\"'; \"\"\"\n",
    "            cursor.execute(sql)\n",
    "            results = cursor.fetchall()\n",
    "            average = results[0][\"Average\"]\n",
    "    except Exception as err:\n",
    "        print(err)\n",
    "        average = None\n",
    "    return average\n",
    "\n",
    "## Generating a report:\n",
    "# we already have the chromosome_regexs list\n",
    "#chromosome_regexs = [\"AT\" + str(num + 1) + \"G[0-9]{5}\" for num in range(5)]\n",
    "\n",
    "average_lengths = {} # chromosome number : average protein length\n",
    "for chr_regex in chromosome_regexs:\n",
    "    average = mean_of_field_where_regex_in_field(mean_fieldname = \"protein_length\", regex = chr_regex, \n",
    "                                                regex_fieldname = \"locus\", tablename = \"locusgene\")\n",
    "    average_lengths[chr_regex[2]] = average # index 2 of the regex is the chromosome number\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chromosome_regexs = [\"AT\" + str(num + 1) + \"G[0-9]{5}\" for num in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chromosome_regexs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
